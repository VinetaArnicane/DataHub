namespace com.linkedin.job

import com.linkedin.common.ChangeAuditStamps
import com.linkedin.common.DatasetUrn
import com.linkedin.common.JobUrn
import com.linkedin.common.Uri
import com.linkedin.common.VersionTag

/**
 * Spec. for a job. A collection of data conforming to a single schema that can evolve over time. This is equivalent to a Table in most data platforms. Espresso dataset: Identity.Profile; oracle dataset: member2.member_profile; hdfs dataset: /data/databases/JOBS/JOB_APPLICATIONS; kafka: PageViewEvent
 */
record Job includes JobKey, ChangeAuditStamps, VersionTag {

  /**
   * Job unique identifier. System assigned value when a new Job is created.
   */
  id: long = 0

  /**
   * Job urn
   */
  urn: JobUrn

  /**
   * Description for Job
   */
  description: string = ""

  /**
   * The native format for the data platform
   */
  platformNativeType: optional enum PlatformNativeType {

    /**
     * Table
     */
    TABLE

    /**
     * View
     */
    VIEW

    /**
     * Directory in file system
     */
    DIRECTORY

    /**
     * Stream
     */
    STREAM

    /**
     * Bucket in key value store
     */
    BUCKET
  }

  /**
   * The abstracted such as hdfs:///data/tracking/PageViewEvent, file:///dir/file_name. Uri should not include any environment specific properties. Some Jobs might not have a standardized uri, which makes this field optional (i.e. kafka topic).
   */
  uri: optional Uri

  /**
   * input datasets of this job
   */
  inputs: optional array[DatasetUrn]

  /**
   * output datasets of this job
   */
  outputs: optional array[DatasetUrn]
}
import logging
from typing import Any, Dict, Iterable, List, Optional, Set, Tuple, Type, Union, cast

# These imports verify that the dependencies are available.
import psycopg2  # noqa: F401
import redshift_connector
import sqlalchemy_redshift  # noqa: F401
from sqlalchemy.engine.reflection import Inspector

from datahub.emitter.mce_builder import (
    make_container_urn,
    make_data_platform_urn,
    make_dataplatform_instance_urn,
    make_dataset_urn,
    make_domain_urn,
    make_tag_urn,
)
from datahub.emitter.mcp_builder import (
    DatabaseKey,
    PlatformKey,
    SchemaKey,
    add_dataset_to_container,
    add_domain_to_entity_wu,
    gen_containers,
    wrap_aspect_as_workunit,
)
from datahub.ingestion.api.common import PipelineContext
from datahub.ingestion.api.decorators import (
    SourceCapability,
    SupportStatus,
    capability,
    config_class,
    platform_name,
    support_status,
)
from datahub.ingestion.api.source import (
    TestableSource,
    TestConnectionReport,
)
from datahub.ingestion.api.workunit import MetadataWorkUnit
from datahub.ingestion.source.redshift.config import RedshiftConfig
from datahub.ingestion.source.redshift.lineage import LineageExtractor
from datahub.ingestion.source.redshift.profile import RedshiftProfiler
from datahub.ingestion.source.redshift.redshift_schema import (
    RedshiftColumn,
    RedshiftDataDictionary,
    RedshiftSchema,
    RedshiftTable,
    RedshiftView,
)
from datahub.ingestion.source.redshift.report import RedshiftReport
from datahub.ingestion.source.redshift.state import RedshiftCheckpointState
from datahub.ingestion.source.redshift.usage import RedshiftUsageExtractor
from datahub.ingestion.source.sql.sql_common import SqlWorkUnit
from datahub.ingestion.source.sql.sql_types import resolve_postgres_modified_type
from datahub.ingestion.source.state.stale_entity_removal_handler import (
    StaleEntityRemovalHandler,
)
from datahub.ingestion.source.state.stateful_ingestion_base import (
    StatefulIngestionSourceBase,
)
from datahub.metadata.com.linkedin.pegasus2avro.common import Status, SubTypes
from datahub.metadata.com.linkedin.pegasus2avro.dataset import (
    DatasetProperties,
    UpstreamLineage,
    ViewProperties,
)

# TRICKY: it's necessary to import the Postgres source because
# that module has some side effects that we care about here.
from datahub.metadata.com.linkedin.pegasus2avro.schema import (
    ArrayType,
    BooleanType,
    BytesType,
    MySqlDDL,
    NullType,
    NumberType,
    RecordType,
    SchemaField,
    SchemaFieldDataType,
    SchemaMetadata,
    StringType,
    TimeType,
)
from datahub.metadata.schema_classes import (
    DataPlatformInstanceClass,
    GlobalTagsClass,
    TagAssociationClass,
)
from datahub.specific.dataset import DatasetPatchBuilder
from datahub.utilities.mapping import Constants
from datahub.utilities.registries.domain_registry import DomainRegistry
from datahub.utilities.source_helpers import (
    auto_stale_entity_removal,
    auto_status_aspect,
)

logger: logging.Logger = logging.getLogger(__name__)


@platform_name("Redshift")
@config_class(RedshiftConfig)
@support_status(SupportStatus.CERTIFIED)
@capability(SourceCapability.PLATFORM_INSTANCE, "Enabled by default")
@capability(SourceCapability.DOMAINS, "Supported via the `domain` config field")
@capability(SourceCapability.DATA_PROFILING, "Optionally enabled via configuration")
@capability(SourceCapability.DESCRIPTIONS, "Enabled by default")
@capability(SourceCapability.LINEAGE_COARSE, "Optionally enabled via configuration")
@capability(
    SourceCapability.USAGE_STATS,
    "Not provided by this module, use `redshift-usage` for that.",
    supported=False,
)
@capability(SourceCapability.DELETION_DETECTION, "Enabled via stateful ingestion")
class RedshiftSource(StatefulIngestionSourceBase, TestableSource):
    """
    This plugin extracts the following:

    - Metadata for databases, schemas, views and tables
    - Column types associated with each table
    - Also supports PostGIS extensions
    - Table, row, and column statistics via optional SQL profiling
    - Table lineage

    :::tip

    You can also get fine-grained usage statistics for Redshift using the `redshift-usage` source described below.

    :::

    ### Prerequisites

    This source needs to access system tables that require extra permissions.
    To grant these permissions, please alter your datahub Redshift user the following way:
    ```sql
    ALTER USER datahub_user WITH SYSLOG ACCESS UNRESTRICTED;
    GRANT SELECT ON pg_catalog.svv_table_info to datahub_user;
    GRANT SELECT ON pg_catalog.svl_user_info to datahub_user;
    ```
    :::note

    Giving a user unrestricted access to system tables gives the user visibility to data generated by other users. For example, STL_QUERY and STL_QUERYTEXT contain the full text of INSERT, UPDATE, and DELETE statements.

    :::

    ### Lineage

    There are multiple lineage collector implementations as Redshift does not support table lineage out of the box.

    #### stl_scan_based
    The stl_scan based collector uses Redshift's [stl_insert](https://docs.aws.amazon.com/redshift/latest/dg/r_STL_INSERT.html) and [stl_scan](https://docs.aws.amazon.com/redshift/latest/dg/r_STL_SCAN.html) system tables to
    discover lineage between tables.
    Pros:
    - Fast
    - Reliable

    Cons:
    - Does not work with Spectrum/external tables because those scans do not show up in stl_scan table.
    - If a table is depending on a view then the view won't be listed as dependency. Instead the table will be connected with the view's dependencies.

    #### sql_based
    The sql_based based collector uses Redshift's [stl_insert](https://docs.aws.amazon.com/redshift/latest/dg/r_STL_INSERT.html) to discover all the insert queries
    and uses sql parsing to discover the dependecies.

    Pros:
    - Works with Spectrum tables
    - Views are connected properly if a table depends on it

    Cons:
    - Slow.
    - Less reliable as the query parser can fail on certain queries

    #### mixed
    Using both collector above and first applying the sql based and then the stl_scan based one.

    Pros:
    - Works with Spectrum tables
    - Views are connected properly if a table depends on it
    - A bit more reliable than the sql_based one only

    Cons:
    - Slow
    - May be incorrect at times as the query parser can fail on certain queries

    :::note

    The redshift stl redshift tables which are used for getting data lineage only retain approximately two to five days of log history. This means you cannot extract lineage from queries issued outside that window.

    :::

    """

    REDSHIFT_FIELD_TYPE_MAPPINGS: Dict[
        str,
        Type[
            Union[
                ArrayType,
                BytesType,
                BooleanType,
                NumberType,
                RecordType,
                StringType,
                TimeType,
                NullType,
            ]
        ],
    ] = {
        "BYTES": BytesType,
        "BOOL": BooleanType,
        "DECIMAL": NumberType,
        "NUMERIC": NumberType,
        "BIGNUMERIC": NumberType,
        "BIGDECIMAL": NumberType,
        "FLOAT64": NumberType,
        "INT": NumberType,
        "INT64": NumberType,
        "SMALLINT": NumberType,
        "INTEGER": NumberType,
        "BIGINT": NumberType,
        "TINYINT": NumberType,
        "BYTEINT": NumberType,
        "STRING": StringType,
        "TIME": TimeType,
        "TIMESTAMP": TimeType,
        "DATE": TimeType,
        "DATETIME": TimeType,
        "GEOGRAPHY": NullType,
        "JSON": NullType,
        "INTERVAL": NullType,
        "ARRAY": ArrayType,
        "STRUCT": RecordType,
        "CHARACTER VARYING": StringType,
        "CHARACTER": StringType,
        "CHAR": StringType,
        "TIMESTAMP WITHOUT TIME ZONE": TimeType,
    }

    def get_platform_instance_id(self) -> str:
        """
        The source identifier such as the specific source host address required for stateful ingestion.
        Individual subclasses need to override this method appropriately.
        """
        return f"{self.platform}"

    @staticmethod
    def test_connection(config_dict: dict) -> TestConnectionReport:
        pass

    def get_report(self) -> RedshiftReport:
        return self.report

    eskind_to_platform = {1: "glue", 2: "hive", 3: "postgres", 4: "redshift"}

    def __init__(self, config: RedshiftConfig, ctx: PipelineContext):
        super(RedshiftSource, self).__init__(config, ctx)
        # super().__init__(config, ctx, "redshift")
        self.lineage_extractor = None
        self.catalog_metadata: Dict = {}
        self.config: RedshiftConfig = config
        self._all_tables_set: Optional[Set[str]] = None
        self.report: RedshiftReport = RedshiftReport()
        self.platform = "redshift"
        # Create and register the stateful ingestion use-case handler.
        self.stale_entity_removal_handler = StaleEntityRemovalHandler(
            source=self,
            config=self.config,
            state_type_class=RedshiftCheckpointState,
            pipeline_name=self.ctx.pipeline_name,
            run_id=self.ctx.run_id,
        )
        if self.config.domain:
            self.domain_registry = DomainRegistry(
                cached_domains=[k for k in self.config.domain], graph=self.ctx.graph
        )

        self.db_tables: Dict[str, Dict[str, List[RedshiftTable]]] = {}
        self.db_views: Dict[str, Dict[str, List[RedshiftView]]] = {}
        self.schema_columns: Dict[str, Dict[str, Dict[str, List[RedshiftColumn]]]] = {}

    @classmethod
    def create(cls, config_dict, ctx):
        config = RedshiftConfig.parse_obj(config_dict)
        return cls(config, ctx)

    def get_redshift_connection(self) -> redshift_connector.Connection:
        client_options = self.config.extra_client_options
        host, port = self.config.host_port.split(":")
        return redshift_connector.connect(
            host=host,
            port=int(port),
            user=self.config.username,
            database=self.config.database if self.config.database else "dev",
            password=self.config.password.get_secret_value(),
            **client_options,
        )

    # TODO: Remove to common
    def gen_schema_key(self, db_name: str, schema: str) -> PlatformKey:
        return SchemaKey(
            database=db_name,
            schema=schema,
            platform=self.platform,
            instance=self.config.platform_instance,
            backcompat_instance_for_guid=self.config.env,
        )

    # TODO: Remove to common
    def gen_database_key(self, database: str) -> PlatformKey:
        return DatabaseKey(
            database=database,
            platform=self.platform,
            instance=self.config.platform_instance,
            backcompat_instance_for_guid=self.config.env,
        )

    # TODO: Remove to common
    def gen_schema_containers(
        self, schema: str, database: str
    ) -> Iterable[MetadataWorkUnit]:
        database_container_key = self.gen_database_key(database)
        schema_container_key = self.gen_schema_key(db_name=database, schema=schema)

        container_workunits = gen_containers(
            schema_container_key,
            schema,
            ["Schema"],
            database_container_key,
        )

        for wu in container_workunits:
            self.report.report_workunit(wu)
            yield wu

    # TODO: Remove to common
    def gen_database_containers(self, database: str) -> Iterable[MetadataWorkUnit]:
        domain_urn = self._gen_domain_urn(database)

        database_container_key = self.gen_database_key(database)

        container_workunits = gen_containers(
            container_key=database_container_key,
            name=database,
            sub_types=["Database"],
            domain_urn=domain_urn,
        )

        self.stale_entity_removal_handler.add_entity_to_state(
            type="container",
            urn=make_container_urn(
                guid=database_container_key.guid(),
            ),
        )

        for wu in container_workunits:
            self.report.report_workunit(wu)
            yield wu

    def process_schema(
        self,
        connection: redshift_connector.Connection,
        database: str,
        schema: RedshiftSchema,
    ):
        schema_workunits = self.gen_schema_containers(
            schema=schema.name,
            database=schema.database,
        )

        for wu in schema_workunits:
            self.report.report_workunit(wu)
            yield wu

        self.schema_columns[database][
            schema.name
        ] = RedshiftDataDictionary.get_columns_for_schema(
            conn=connection, schema=schema
        )

        if self.config.include_tables:
            logger.info("process tables")
            if not self.db_tables[schema.database]:
                return

            if schema.name in self.db_tables[schema.database]:
                for table in self.db_tables[schema.database][schema.name]:
                    logger.info(f"Table: {table}")
                    table.columns = (
                        self.schema_columns[database][schema.name][table.name]
                        if table.name in self.schema_columns[database][schema.name]
                        else []
                    )
                    yield from self._process_table(
                        table, database=database, schema=schema
                    )

        if self.config.include_views:
            logger.info("process views")
            if schema.name in self.db_views[schema.database]:
                for table in self.db_views[schema.database][schema.name]:
                    logger.info(f"View: {table}")
                    table.columns = (
                        self.schema_columns[database][schema.name][table.name]
                        if table.name in self.schema_columns[database][schema.name]
                        else []
                    )
                    yield from self._process_view(table, database, schema)

    def _process_table(
        self,
        table: RedshiftTable,
        schema: RedshiftSchema,
        database: str,
    ) -> Iterable[MetadataWorkUnit]:

        datahub_dataset_name = f"{database}.{table.schema}.{table.name}"

        self.report.report_entity_scanned(datahub_dataset_name)

        if not self.config.table_pattern.allowed(datahub_dataset_name):
            self.report.report_dropped(datahub_dataset_name)
            return

        # lineage_info: Optional[Tuple[UpstreamLineage, Dict[str, str]]] = None
        if self.config.include_table_lineage:
            dataset_urn = make_dataset_urn(
                platform=self.platform, name=datahub_dataset_name
            )

            assert self.lineage_extractor
            lineage_mcp = self.lineage_extractor.get_lineage_mcp(
                schema=schema,
                table=table,
                connection=self.get_redshift_connection(),
                dataset_urn=dataset_urn,
            )
            if lineage_mcp is not None:
                lineage_wu = MetadataWorkUnit(
                    id=f"redshift-{lineage_mcp.entityUrn}-{lineage_mcp.aspectName}",
                    mcp=lineage_mcp,
                )
                self.report.report_workunit(lineage_wu)

                yield lineage_wu
        # if self.config.include_table_lineage:
        #    lineage_info = self.lineage_extractor.get_upstream_lineage_info(
        #        project_id=project_id,
        #        dataset_name=schema_name,
        #        table=table,
        #        platform=self.platform,
        #    )

        table_workunits = self.gen_table_dataset_workunits(
            table,
            database=database,
            dataset_name=datahub_dataset_name,
            lineage_info=None,
        )
        for wu in table_workunits:
            self.report.report_workunit(wu)
            yield wu

    def _process_view(
        self, table: RedshiftView, database: str, schema: RedshiftSchema
    ) -> Iterable[MetadataWorkUnit]:

        datahub_dataset_name = f"{database}.{table.schema}.{table.name}"

        self.report.report_entity_scanned(datahub_dataset_name)

        if not self.config.table_pattern.allowed(datahub_dataset_name):
            self.report.report_dropped(datahub_dataset_name)
            return

        # lineage_info: Optional[Tuple[UpstreamLineage, Dict[str, str]]] = None

        # if self.config.include_table_lineage:
        #    lineage_info = self.lineage_extractor.get_upstream_lineage_info(
        #        project_id=project_id,
        #        dataset_name=schema_name,
        #        table=table,
        #        platform=self.platform,
        #    )

        table_workunits = self.gen_view_dataset_workunits(
            table,
            database=database,
            schema=table.schema,
            lineage_info=None,
        )

        for wu in table_workunits:
            self.report.report_workunit(wu)
            yield wu

        if self.config.include_table_lineage:
            dataset_urn = make_dataset_urn(
                platform=self.platform, name=datahub_dataset_name
            )

            assert self.lineage_extractor
            lineage_mcp = self.lineage_extractor.get_lineage_mcp(
                schema=schema,
                table=table,
                connection=self.get_redshift_connection(),
                dataset_urn=dataset_urn,
            )
            if lineage_mcp is not None:
                lineage_wu = MetadataWorkUnit(
                    id=f"redshift-{lineage_mcp.entityUrn}-{lineage_mcp.aspectName}",
                    mcp=lineage_mcp,
                )
                self.report.report_workunit(lineage_wu)

                yield lineage_wu

    def gen_table_dataset_workunits(
        self,
        table: RedshiftTable,
        database: str,
        dataset_name: str,
        lineage_info: Optional[Tuple[UpstreamLineage, Dict[str, str]]],
    ) -> Iterable[MetadataWorkUnit]:

        custom_properties = {}

        if table.location:
            custom_properties["location"] = table.location

        if table.input_parameters:
            custom_properties["input_parameters"] = table.input_parameters

        if table.output_parameters:
            custom_properties["output_parameters"] = table.output_parameters

        if table.dist_style:
            custom_properties["dist_style"] = table.dist_style

        if table.parameters:
            custom_properties["parameters"] = table.parameters

        if table.serde_parameters:
            custom_properties["serde_parameters"] = table.serde_parameters

        yield from self.gen_dataset_workunits(
            table=table,
            database=database,
            schema=table.schema,
            sub_type=table.type,
            lineage_info=lineage_info,
            tags_to_add=[],
            custom_properties=custom_properties,
        )

    # TODO: Remove to common?
    def gen_view_dataset_workunits(
        self,
        table: RedshiftView,
        database: str,
        schema: str,
        lineage_info: Optional[Tuple[UpstreamLineage, Dict[str, str]]],
    ) -> Iterable[MetadataWorkUnit]:

        view = cast(RedshiftView, table)

        yield from self.gen_dataset_workunits(
            table=table,
            database=database,
            schema=table.schema,
            sub_type=table.type,
            lineage_info=lineage_info,
            tags_to_add=[],
            custom_properties={},
        )

        view_definition_string = view.ddl
        view_properties_aspect = ViewProperties(
            materialized=table.type == "VIEW_MATERIALIZED",
            viewLanguage="SQL",
            viewLogic=view_definition_string,
        )
        datahub_dataset_name = f"{database}.{schema}.{table.name}"
        dataset_urn = make_dataset_urn(
            platform=self.platform, name=datahub_dataset_name
        )
        wu = wrap_aspect_as_workunit(
            "dataset",
            dataset_urn,
            "viewProperties",
            view_properties_aspect,
        )
        yield wu
        self.report.report_workunit(wu)

    # TODO: Remove to common?
    def gen_schema_fields(self, columns: List[RedshiftColumn]) -> List[SchemaField]:
        schema_fields: List[SchemaField] = []

        for col in columns:
            tags: List[TagAssociationClass] = []
            if col.dist_key:
                tags.append(TagAssociationClass(make_tag_urn(Constants.TAG_DIST_KEY)))

            if col.sort_key:
                tags.append(TagAssociationClass(make_tag_urn(Constants.TAG_SORT_KEY)))

            data_type = self.REDSHIFT_FIELD_TYPE_MAPPINGS.get(col.data_type)
            # We have to remove the precision part to properly parse it
            if data_type is None:
                # attempt Postgres modified type
                data_type = resolve_postgres_modified_type(col.data_type.lower())

            field = SchemaField(
                fieldPath=col.name,
                type=SchemaFieldDataType(data_type() if data_type else NullType()),
                # NOTE: nativeDataType will not be in sync with older connector
                nativeDataType=col.data_type,
                description=col.comment,
                nullable=col.is_nullable,
                globalTags=GlobalTagsClass(tags=tags),
            )
            schema_fields.append(field)
        return schema_fields

    # TODO: Remove to common?
    def gen_schema_metadata(
        self,
        dataset_urn: str,
        table: Union[RedshiftTable, RedshiftView],
        dataset_name: str,
    ) -> MetadataWorkUnit:

        schema_metadata = SchemaMetadata(
            schemaName=dataset_name,
            platform=make_data_platform_urn(self.platform),
            version=0,
            hash="",
            platformSchema=MySqlDDL(tableSchema=""),
            fields=self.gen_schema_fields(table.columns),
        )
        wu = wrap_aspect_as_workunit(
            "dataset", dataset_urn, "schemaMetadata", schema_metadata
        )
        self.report.report_workunit(wu)
        return wu

    # TODO: Remove to common
    def gen_dataset_key(self, db_name: str, schema: str) -> PlatformKey:
        return SchemaKey(
            database=db_name,
            schema=schema,
            platform=self.platform,
            instance=self.config.platform_instance,
            backcompat_instance_for_guid=self.config.env,
        )

    # TODO: Remove to common
    def add_table_to_dataset_container(
        self, dataset_urn: str, db_name: str, schema: str
    ) -> Iterable[MetadataWorkUnit]:
        schema_container_key = self.gen_dataset_key(db_name, schema)
        container_workunits = add_dataset_to_container(
            container_key=schema_container_key,
            dataset_urn=dataset_urn,
        )
        for wu in container_workunits:
            self.report.report_workunit(wu)
            yield wu

    # TODO: Remove to common
    def get_dataplatform_instance_aspect(
        self, dataset_urn: str
    ) -> Optional[MetadataWorkUnit]:
        # If we are a platform instance based source, emit the instance aspect
        if self.config.platform_instance:
            aspect = DataPlatformInstanceClass(
                platform=make_data_platform_urn(self.platform),
                instance=make_dataplatform_instance_urn(
                    self.platform, self.config.platform_instance
                ),
            )

            return wrap_aspect_as_workunit(
                "dataset", dataset_urn, "dataPlatformInstance", aspect
            )
        else:
            return None

    # TODO: Remove to common
    def _gen_domain_urn(self, dataset_name: str) -> Optional[str]:
        domain_urn: Optional[str] = None

        for domain, pattern in self.config.domain.items():
            if pattern.allowed(dataset_name):
                domain_urn = make_domain_urn(
                    self.domain_registry.get_domain_urn(domain)
                )

        return domain_urn

    # TODO: Remove to common
    def _get_domain_wu(
        self,
        dataset_name: str,
        entity_urn: str,
        entity_type: str,
    ) -> Iterable[MetadataWorkUnit]:

        domain_urn = self._gen_domain_urn(dataset_name)
        if domain_urn:
            wus = add_domain_to_entity_wu(
                entity_type=entity_type,
                entity_urn=entity_urn,
                domain_urn=domain_urn,
            )
            for wu in wus:
                self.report.report_workunit(wu)
                yield wu

    def gen_dataset_workunits(
        self,
        table: Union[RedshiftTable, RedshiftView],
        database: str,
        schema: str,
        sub_type: str,
        lineage_info: Optional[Tuple[UpstreamLineage, Dict[str, str]]] = None,
        tags_to_add: Optional[List[str]] = None,
        custom_properties: Optional[Dict[str, str]] = None,
    ) -> Iterable[MetadataWorkUnit]:
        datahub_dataset_name = f"{database}.{schema}.{table.name}"
        dataset_urn = make_dataset_urn(
            platform=self.platform, name=datahub_dataset_name
        )
        status = Status(removed=False)
        wu = wrap_aspect_as_workunit("dataset", dataset_urn, "status", status)
        yield wu
        self.report.report_workunit(wu)

        yield self.gen_schema_metadata(dataset_urn, table, str(datahub_dataset_name))

        if lineage_info is not None:
            upstream_lineage, upstream_column_props = lineage_info
        else:
            upstream_column_props = {}
            upstream_lineage = None

        if upstream_lineage is not None:
            if self.config.incremental_lineage:
                patch_builder: DatasetPatchBuilder = DatasetPatchBuilder(
                    urn=dataset_urn
                )
                for upstream in upstream_lineage.upstreams:
                    patch_builder.add_upstream_lineage(upstream)

                lineage_workunits = [
                    MetadataWorkUnit(
                        id=f"upstreamLineage-for-{dataset_urn}",
                        mcp_raw=mcp,
                    )
                    for mcp in patch_builder.build()
                ]
            else:
                lineage_workunits = [
                    wrap_aspect_as_workunit(
                        "dataset", dataset_urn, "upstreamLineage", upstream_lineage
                    )
                ]

            for wu in lineage_workunits:
                yield wu
                self.report.report_workunit(wu)

        dataset_properties = DatasetProperties(
            name=table.name,
            description=table.description,
            qualifiedName=str(datahub_dataset_name),
            customProperties={**upstream_column_props},
        )
        if custom_properties:
            dataset_properties.customProperties.update(custom_properties)

        wu = wrap_aspect_as_workunit(
            "dataset", dataset_urn, "datasetProperties", dataset_properties
        )
        yield wu
        self.report.report_workunit(wu)

        #TODO: Check if needed
        #if tags_to_add:
        #    yield self.gen_tags_aspect_workunit(dataset_urn, tags_to_add)

        yield from self.add_table_to_dataset_container(
            dataset_urn,
            database,
            schema,
        )
        dpi_aspect = self.get_dataplatform_instance_aspect(dataset_urn=dataset_urn)
        if dpi_aspect:
            self.report.report_workunit(dpi_aspect)
            yield dpi_aspect

        subTypes = SubTypes(typeNames=[sub_type])
        wu = wrap_aspect_as_workunit("dataset", dataset_urn, "subTypes", subTypes)
        yield wu
        self.report.report_workunit(wu)

        yield from self._get_domain_wu(
            dataset_name=str(datahub_dataset_name),
            entity_urn=dataset_urn,
            entity_type="dataset",
        )

    def get_workunits_internal(self) -> Iterable[Union[MetadataWorkUnit, SqlWorkUnit]]:
        connection = self.get_redshift_connection()
        logger.info("Getting projects")
        # self.add_config_to_report()

        databases: List[str] = RedshiftDataDictionary.get_databases(connection)
        for database in databases:
            logger.info(f"Database {database}")
            self.db_tables[database] = {}
            self.db_views[database] = {}
            self.schema_columns[database] = {}

            tables, views = RedshiftDataDictionary.get_tables_and_views(conn=connection)
            self.db_tables[database].update(tables)
            self.db_views[database].update(views)

            if self.config.include_table_lineage or self.config.include_copy_lineage:
                self.lineage_extractor = LineageExtractor(
                    config=self.config,
                    report=self.report,
                    all_tables={**self.db_tables, **self.db_views},
                )

            for schema in RedshiftDataDictionary.get_schemas(
                conn=connection, database=database
            ):
                logger.info(f"Schema: {schema}")
                if not self.config.schema_pattern.allowed(schema.name):
                    continue
                yield from self.process_schema(connection, database, schema)

            if self.config.include_usage_statistics:
                usage_extractor = RedshiftUsageExtractor(
                    config=self.config,
                    connection=self.get_redshift_connection(),
                    report=self.report,
                )
                yield from usage_extractor.generate_usage()

            if self.config.profiling.enabled:
                profiler = RedshiftProfiler(config=self.config, report=self.report)
                yield from profiler.get_workunits(self.db_tables)

    def get_workunits(self) -> Iterable[MetadataWorkUnit]:
        return auto_stale_entity_removal(
            self.stale_entity_removal_handler,
            auto_status_aspect(self.get_workunits_internal()),
        )

    # TODO: Remove to common
    def get_db_name(self, inspector: Inspector = None) -> str:
        db_name = getattr(self.config, "database")
        db_alias = getattr(self.config, "database_alias")
        if db_alias:
            db_name = db_alias
        return db_name

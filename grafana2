import logging
import re

from dataclasses import dataclass
from typing import Dict, Iterable, List, Optional, Set

import requests
from pydantic import Field, SecretStr, validator

import datahub.emitter.mce_builder as builder
from datahub.configuration.source_common import DatasetLineageProviderConfigBase
from datahub.configuration.common import ConfigModel
from datahub.emitter.mcp import MetadataChangeProposalWrapper
from datahub.emitter.mcp_builder import ContainerKey, add_dataset_to_container, gen_containers
from datahub.ingestion.api.common import PipelineContext
from datahub.ingestion.api.decorators import (
    SourceCapability,
    SupportStatus,
    capability,
    config_class,
    platform_name,
    support_status,
)
from datahub.ingestion.api.source import MetadataWorkUnitProcessor
from datahub.ingestion.api.workunit import MetadataWorkUnit
from datahub.ingestion.source.state.stale_entity_removal_handler import (
    StaleEntityRemovalHandler,
    StaleEntityRemovalSourceReport,
    StatefulStaleMetadataRemovalConfig,
)
from datahub.ingestion.source.state.stateful_ingestion_base import (
    StatefulIngestionConfigBase,
    StatefulIngestionSourceBase,
)
from datahub.metadata.com.linkedin.pegasus2avro.mxe import MetadataChangeEvent
from datahub.metadata.com.linkedin.pegasus2avro.schema import OtherSchema
from datahub.metadata.schema_classes import (
    BrowsePathsClass,
    ChangeAuditStampsClass,
    ChartInfoClass,
    ChartSnapshotClass,
    ChartTypeClass,
    DashboardSnapshotClass,
    DataPlatformInstanceClass,
    DashboardInfoClass,
    DatasetPropertiesClass,
    DatasetSnapshotClass,
    GlobalTagsClass,
    InputFieldClass,
    InputFieldsClass,
    OwnerClass,
    OwnershipClass,
    OwnershipTypeClass,
    StatusClass,
    SubTypesClass,
    TagAssociationClass,
    BooleanTypeClass,
    NumberTypeClass,
    StringTypeClass,
    TimeTypeClass,
    DateTypeClass,
    SchemaFieldClass,
    SchemaFieldDataTypeClass,
    SchemaMetadataClass,
    FineGrainedLineageClass,
    FineGrainedLineageDownstreamTypeClass,
    FineGrainedLineageUpstreamTypeClass,
    UpstreamLineageClass,
    DatasetLineageTypeClass,
    UpstreamClass,
)
from datahub.sql_parsing.sqlglot_lineage import create_lineage_sql_parsed_result, SqlParsingResult
from datahub.utilities import config_clean

logger = logging.getLogger(__name__)


class PlatformConnectionConfig(ConfigModel):
    platform: str = Field(description="Platform to connect to")
    database: Optional[str] = Field(default=None, description="Database name")
    platform_instance: Optional[str] = Field(default=None, description="Platform instance")
    env: Optional[str] = Field(default=None, description="Environment")


class GrafanaSourceConfig(StatefulIngestionConfigBase, DatasetLineageProviderConfigBase):
    platform: str = Field(default="grafana", hidden_from_docs=True)
    url: str = Field(description="URL of Grafana instance (e.g. https://grafana.company.com)")
    service_account_token: SecretStr = Field(description="Grafana API token")
    platform_instance: Optional[str] = Field(
        default=None,
        description="Platform instance for DataHub"
    )
    auto_tag_dimensions: bool = Field(
        default=False,
        description="Automatically tag dimension fields in charts",
    )
    auto_tag_measures: bool = Field(
        default=False,
        description="Automatically tag measure fields in charts",
    )
    connection_to_platform_map: Dict[str, PlatformConnectionConfig] = Field(
        default={},
        description="Map of Grafana connection names to their upstream platform details"
    )
    stateful_ingestion: Optional[StatefulStaleMetadataRemovalConfig] = None

    @validator("url")
    def remove_trailing_slash(cls, v):
        return config_clean.remove_trailing_slashes(v)


class GrafanaTypeMapper:
    """Maps Grafana types to DataHub types"""

    _TYPE_MAPPINGS = {
        "string": SchemaFieldDataTypeClass(type=StringTypeClass()),
        "number": SchemaFieldDataTypeClass(type=NumberTypeClass()),
        "integer": SchemaFieldDataTypeClass(type=NumberTypeClass()),
        "float": SchemaFieldDataTypeClass(type=NumberTypeClass()),
        "boolean": SchemaFieldDataTypeClass(type=BooleanTypeClass()),
        "time": SchemaFieldDataTypeClass(type=TimeTypeClass()),
        "timestamp": SchemaFieldDataTypeClass(type=TimeTypeClass()),
        "timeseries": SchemaFieldDataTypeClass(type=TimeTypeClass()),
        "time_series": SchemaFieldDataTypeClass(type=TimeTypeClass()),
        "datetime": SchemaFieldDataTypeClass(type=TimeTypeClass()),
        "date": SchemaFieldDataTypeClass(type=DateTypeClass()),
    }

    @classmethod
    def get_field_type(cls, grafana_type: str, default_type: str = "string") -> SchemaFieldDataTypeClass:
        return cls._TYPE_MAPPINGS.get(
            grafana_type.lower(),
            cls._TYPE_MAPPINGS.get(default_type, cls._TYPE_MAPPINGS["string"])
        )

    @classmethod
    def get_native_type(cls, grafana_type: str, default_type: str = "string") -> str:
        grafana_type = grafana_type.lower()
        if grafana_type in cls._TYPE_MAPPINGS:
            return grafana_type
        return default_type


class FolderKey(ContainerKey):
    folder_id: str

class DashboardContainerKey(ContainerKey):
    dashboard_id: str

class SubTypes:
    VIEW = "View"
    METRICS = "Metrics"

@dataclass
class GrafanaSourceReport(StaleEntityRemovalSourceReport):
    dashboards_scanned: int = 0
    charts_scanned: int = 0
    folders_scanned: int = 0
    datasets_scanned: int = 0

    def report_dashboard_scanned(self) -> None:
        self.dashboards_scanned += 1

    def report_chart_scanned(self) -> None:
        self.charts_scanned += 1

    def report_folder_scanned(self) -> None:
        self.folders_scanned += 1

    def report_dataset_scanned(self) -> None:
        self.datasets_scanned += 1


@platform_name("Grafana")
@config_class(GrafanaSourceConfig)
@support_status(SupportStatus.CERTIFIED)
@capability(SourceCapability.PLATFORM_INSTANCE, "Enabled by default")
@capability(SourceCapability.DELETION_DETECTION, "Enabled by default")
@capability(SourceCapability.LINEAGE_COARSE, "Enabled by default")
@capability(SourceCapability.LINEAGE_FINE, "Enabled by default")
@capability(SourceCapability.OWNERSHIP, "Enabled by default")
@capability(SourceCapability.TAGS, "Enabled by default")
class GrafanaSource(StatefulIngestionSourceBase):
    """
    This source extracts the following:
    - Folders
    - Dashboards
    - Charts
    - Tags
    - Owners
    - Lineage information
    """
    config: GrafanaSourceConfig
    report: GrafanaSourceReport

    def __init__(self, config: GrafanaSourceConfig, ctx: PipelineContext):
        super().__init__(config, ctx)
        self.config = config
        self.ctx = ctx
        self.platform = config.platform
        self.platform_instance = self.config.platform_instance
        self.env = self.config.env
        self.report = GrafanaSourceReport()
        self.session = self.set_session()

    @classmethod
    def create(cls, config_dict: dict, ctx: PipelineContext) -> "GrafanaSource":
        config = GrafanaSourceConfig.parse_obj(config_dict)
        return cls(config, ctx)

    def set_session(self) -> requests.Session:
        session = requests.Session()
        session.headers.update({
            "Authorization": f"Bearer {self.config.service_account_token.get_secret_value()}",
            "Accept": "application/json",
            "Content-Type": "application/json",
        })
        return session

    def get_workunit_processors(self) -> List[Optional[MetadataWorkUnitProcessor]]:
        return [
            *super().get_workunit_processors(),
            StaleEntityRemovalHandler.create(
                self, self.config, self.ctx
            ).workunit_processor,
        ]

    def get_workunits_internal(self) -> Iterable[MetadataWorkUnit]:
        # Emit folder containers first
        folders = self._get_folders()
        for folder in folders:
            self.report.report_folder_scanned()
            yield from self._emit_folder(folder)

        # Track charts by dashboard
        dashboard_chart_map: Dict[str, List[str]] = {}

        # First collect and emit all datasources to ensure they exist
        datasource_map: Dict[str, Set[str]] = {}  # dashboard_uid -> set of datasource UIDs
        dashboards = self._get_dashboards()
        for dashboard in dashboards:
            dashboard_uid = dashboard.get("dashboard", {}).get("uid")
            if not dashboard_uid:
                continue

            datasource_map[dashboard_uid] = set()
            for panel in self._get_panels(dashboard):
                datasource = panel.get("datasource")
                if isinstance(datasource, dict):
                    ds_uid = datasource.get("uid")
                    if ds_uid and ds_uid not in datasource_map[dashboard_uid]:
                        datasource_map[dashboard_uid].add(ds_uid)
                        # Emit datasource first and only once per dashboard
                        yield from self._emit_datasource(panel, dashboard_uid)

        # Then emit dashboards and charts
        for dashboard in dashboards:
            self.report.report_dashboard_scanned()
            dashboard_uid = dashboard.get("dashboard", {}).get("uid")
            if not dashboard_uid:
                continue

            dashboard_chart_map[dashboard_uid] = []

            # Process all panels
            for panel in self._get_panels(dashboard):
                self.report.report_chart_scanned()

                if not panel.get("id"):
                    continue

                chart_urn = builder.make_chart_urn(
                    self.platform,
                    f"{dashboard_uid}.{panel['id']}",
                    self.platform_instance
                )
                dashboard_chart_map[dashboard_uid].append(chart_urn)

                # Emit any dataset-to-dataset lineage
                lineage_workunit = self._emit_lineage(panel)
                if lineage_workunit:
                    yield lineage_workunit

                # Emit chart metadata
                yield from self._emit_chart(panel, dashboard)

            # Finally emit dashboard with all chart URNs
            yield from self._emit_dashboard(dashboard, dashboard_chart_map[dashboard_uid])

    def _get_folders(self) -> List[dict]:
        """Fetch folders from Grafana API"""
        try:
            response = self.session.get(f"{self.config.url}/api/folders")
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            self.report.report_warning(
                message="Failed to fetch folders",
                exc=e,
            )
            return []

    def _get_dashboards(self) -> List[dict]:
        """Fetch dashboards from Grafana API"""
        try:
            response = self.session.get(f"{self.config.url}/api/search?type=dash-db")
            response.raise_for_status()

            dashboards = []
            for result in response.json():
                try:
                    dashboard_response = self.session.get(
                        f"{self.config.url}/api/dashboards/uid/{result['uid']}"
                    )
                    dashboard_response.raise_for_status()
                    dashboards.append(dashboard_response.json())
                except requests.exceptions.RequestException as e:
                    self.report.report_warning(
                        message="Failed to fetch dashboard",
                        context=result['uid'],
                        exc=e,
                    )

            return dashboards
        except requests.exceptions.RequestException as e:
            self.report.report_warning(
                message="Failed to fetch dashboards",
                exc=e,
            )
            return []

    def _get_panels(self, dashboard: dict) -> List[dict]:
        """Extract panels (charts) from dashboard"""
        panels = []
        dashboard_data = dashboard.get("dashboard", {})
        if "panels" in dashboard_data:
            for panel in dashboard_data["panels"]:
                if panel.get("type") not in ["row"]:
                    panels.append(panel)
                # Handle nested panels in rows
                if panel.get("type") == "row" and "panels" in panel:
                    panels.extend([p for p in panel["panels"] if p.get("type") != "row"])
        return panels

    def _emit_folder(self, folder: dict) -> Iterable[MetadataWorkUnit]:
        folder_key = FolderKey(
            platform=self.platform,
            instance=self.platform_instance,
            folder_id=folder["uid"],
        )

        container_urn = builder.make_container_urn(
            guid=folder_key,
        )

        yield MetadataChangeProposalWrapper(
            entityUrn=container_urn,
            aspect=self._create_data_platform_instance(),
        ).as_workunit()

        yield from gen_containers(
            container_key=folder_key,
            name=folder["title"],
            sub_types=["Folder"],
            description=folder.get("description", ""),
        )

    def _emit_dashboard(
        self, dashboard: dict, chart_urns: List[str]
    ) -> Iterable[MetadataWorkUnit]:
        dashboard_data = dashboard.get("dashboard", {})
        dashboard_uid = dashboard_data.get("uid")
        if not dashboard_uid:
            return

        dashboard_urn = builder.make_dashboard_urn(
            self.platform, dashboard_uid, self.platform_instance
        )

        yield from self._create_dashboard_container(
            dashboard_data,
            folder_id=dashboard.get("meta", {}).get("folderId")
        )

        # Add dashboard to container
        dashboard_key = DashboardContainerKey(
            platform=self.platform,
            instance=self.platform_instance,
            dashboard_id=dashboard_uid,
        )
        yield from add_dataset_to_container(
            container_key=dashboard_key,
            dataset_urn=dashboard_urn,
        )

        dashboard_snapshot = DashboardSnapshotClass(
            urn=dashboard_urn,
            aspects=[],
        )

        dashboard_snapshot.aspects.append(
            self._create_data_platform_instance()
        )

        # Basic dashboard info
        title = dashboard_data.get("title", "")
        description = dashboard_data.get("description", "")

        # Last modified info
        last_modified = ChangeAuditStampsClass()

        dashboard_url = f"{self.config.url}/d/{dashboard_uid}"

        # Custom properties
        custom_props = {
            "version": str(dashboard_data.get("version", "")),
            "schemaVersion": str(dashboard_data.get("schemaVersion", "")),
            "timezone": dashboard_data.get("timezone", ""),
        }
        refresh = dashboard_data.get("refresh")
        if refresh:
            custom_props["refresh"] = str(refresh)

        dashboard_info = DashboardInfoClass(
            description=description,
            title=title,
            charts=chart_urns,
            lastModified=last_modified,
            dashboardUrl=dashboard_url,
            customProperties=custom_props,
        )
        dashboard_snapshot.aspects.append(dashboard_info)

        # Add ownership if available
        owner = self._get_owner(dashboard_data)
        if owner:
            dashboard_snapshot.aspects.append(owner)

        # Add tags if present
        if dashboard_data.get("tags"):
            tags = []
            for tag in dashboard_data["tags"]:
                if isinstance(tag, str):
                    tags.append(
                        TagAssociationClass(tag=builder.make_tag_urn(tag))
                    )
            if tags:
                dashboard_snapshot.aspects.append(GlobalTagsClass(tags=tags))

        # Add browse path
        browse_paths = []
        browse_path = f"/grafana"
        if self.platform_instance:
            browse_path += f"/{self.platform_instance}"

        # Add folder to path if present
        folder_id = dashboard.get("meta", {}).get("folderId")
        if folder_id:
            try:
                folder_response = self.session.get(f"{self.config.url}/api/folders/{folder_id}")
                folder_response.raise_for_status()
                folder_data = folder_response.json()
                browse_path += f"/{folder_data['title']}"
            except requests.exceptions.RequestException:
                pass

        browse_path += f"/{title}"
        browse_paths.append(browse_path)

        dashboard_snapshot.aspects.append(
            BrowsePathsClass(paths=browse_paths)
        )

        # Add status
        status = StatusClass(removed=False)
        dashboard_snapshot.aspects.append(status)

        yield MetadataWorkUnit(
            id=f"grafana-dashboard-{dashboard_uid}",
            mce=MetadataChangeEvent(proposedSnapshot=dashboard_snapshot),
        )

        # Handle folder containment
        folder_id = dashboard.get("meta", {}).get("folderId")
        if folder_id:
            folder_key = FolderKey(
                platform=self.platform,
                instance=self.platform_instance,
                folder_id=str(folder_id),
            )
            yield from add_dataset_to_container(
                container_key=folder_key,
                dataset_urn=dashboard_urn,
            )

    def _emit_chart(self, panel: dict, dashboard: dict) -> Iterable[MetadataWorkUnit]:
        dashboard_uid = dashboard.get("dashboard", {}).get("uid")
        if not dashboard_uid or not panel.get("id"):
            return []

        chart_urn = builder.make_chart_urn(
            self.platform,
            f"{dashboard_uid}.{panel['id']}",
            self.platform_instance
        )

        chart_snapshot = ChartSnapshotClass(
            urn=chart_urn,
            aspects=[],
        )

        chart_snapshot.aspects.append(
            self._create_data_platform_instance()
        )

        # Basic chart info - ensure title exists
        title = panel.get("title", "")
        if not title:
            # Use panel ID or type as fallback name
            title = f"Panel {panel.get('id', '')}" if panel.get('id') else panel.get('type', 'Unnamed Panel')

        description = panel.get("description", "")
        chart_type = self._get_chart_type(panel)
        last_modified = ChangeAuditStampsClass()
        chart_url = f"{self.config.url}/d/{dashboard_uid}?viewPanel={panel['id']}"

        # Get input datasets
        input_datasets = []
        datasource = panel.get("datasource")
        if datasource and isinstance(datasource, dict):
            ds_type = datasource.get("type", "unknown")
            ds_uid = datasource.get("uid", "unknown")

            # Add Grafana dataset first - use full path for name
            dataset_name = f"{ds_type}.{ds_uid}"
            ds_urn = builder.make_dataset_urn_with_platform_instance(
                platform=self.platform,
                name=dataset_name,
                platform_instance=self.platform_instance,
                env=self.env,
            )
            input_datasets.append(ds_urn)

            # Add upstream platform dataset if configured
            if self.config.connection_to_platform_map and ds_uid in self.config.connection_to_platform_map:
                platform_config = self.config.connection_to_platform_map[ds_uid]
                name = f"{platform_config.database}.{ds_uid}" if platform_config.database else ds_uid
                upstream_urn = builder.make_dataset_urn_with_platform_instance(
                    platform=platform_config.platform,
                    name=name,
                    platform_instance=platform_config.platform_instance,
                    env=platform_config.env or "PROD",
                )
                input_datasets.append(upstream_urn)

        # Extract custom properties
        custom_props = self._get_custom_properties(panel)

        chart_info = ChartInfoClass(
            type=chart_type,
            description=description,
            title=title,
            lastModified=last_modified,
            chartUrl=chart_url,
            inputs=input_datasets,
            customProperties=custom_props,
        )
        chart_snapshot.aspects.append(chart_info)

        # Add ownership from dashboard
        owner = self._get_owner(dashboard.get("dashboard", {}))
        if owner:
            chart_snapshot.aspects.append(owner)

        # Add status
        status = StatusClass(removed=False)
        chart_snapshot.aspects.append(status)

        # Add browse path
        browse_paths = []
        browse_path = f"/grafana"
        if self.platform_instance:
            browse_path += f"/{self.platform_instance}"

        # Add folder to path if present
        folder_id = dashboard.get("meta", {}).get("folderId")
        if folder_id:
            try:
                folder_response = self.session.get(f"{self.config.url}/api/folders/{folder_id}")
                folder_response.raise_for_status()
                folder_data = folder_response.json()
                browse_path += f"/{folder_data['title']}"
            except requests.exceptions.RequestException:
                pass

        dashboard_title = dashboard.get("dashboard", {}).get("title", "")
        browse_path += f"/{dashboard_title}/{title}"
        browse_paths.append(browse_path)

        chart_snapshot.aspects.append(
            BrowsePathsClass(paths=browse_paths)
        )

        # Input fields - moved before MetadataWorkUnit yield
        input_fields = self._get_chart_fields(panel)
        if input_fields and input_datasets:
            # Always use Grafana dataset for input fields (the first in input_datasets)
            dataset_urn = input_datasets[0]
            yield self._add_input_fields_to_chart(
                chart_urn=chart_urn,
                dataset_urn=dataset_urn,
                input_fields=input_fields,
            )

        yield MetadataWorkUnit(
            id=f"grafana-chart-{dashboard_uid}-{panel['id']}",
            mce=MetadataChangeEvent(proposedSnapshot=chart_snapshot),
        )

        # Add chart to dashboard container
        dashboard_key = DashboardContainerKey(
            platform=self.platform,
            instance=self.platform_instance,
            dashboard_id=dashboard_uid,
        )
        yield from add_dataset_to_container(
            container_key=dashboard_key,
            dataset_urn=chart_urn,
        )

    def _add_input_fields_to_chart(
            self,
            chart_urn: str,
            dataset_urn: str,
            input_fields: List[SchemaFieldClass]
    ) -> Optional[MetadataWorkUnit]:
        """Add input fields aspect to chart"""
        if not input_fields:
            return None

        return MetadataChangeProposalWrapper(
            entityUrn=chart_urn,
            aspect=InputFieldsClass(
                fields=[
                    InputFieldClass(
                        schemaField=field,
                        schemaFieldUrn=builder.make_schema_field_urn(
                            dataset_urn,
                            field.fieldPath
                        )
                    )
                    for field in input_fields
                ]
            )
        ).as_workunit()

    def _emit_datasource(self, panel: dict, dashboard_uid: Optional[str] = None) -> Iterable[MetadataWorkUnit]:
        source = panel.get("datasource")
        if not source or not isinstance(source, dict):
            return

        ds_type = source.get("type", "unknown")
        ds_uid = source.get("uid", "unknown")
        visual_name = panel.get("title", "")

        # Create standardized dataset name
        dataset_name = f"{ds_type}.{ds_uid}.{visual_name}"

        dataset_urn = builder.make_dataset_urn_with_platform_instance(
            platform=self.platform,
            name=dataset_name,
            platform_instance=self.platform_instance,
            env=self.env,
        )

        if dashboard_uid:
            dashboard_key = DashboardContainerKey(
                platform=self.platform,
                instance=self.platform_instance,
                dashboard_id=dashboard_uid,
            )
            yield from add_dataset_to_container(
                container_key=dashboard_key,
                dataset_urn=dataset_urn,
            )

        dataset_snapshot = DatasetSnapshotClass(
            urn=dataset_urn,
            aspects=[
                self._create_data_platform_instance(),
                DatasetPropertiesClass(
                    name=ds_uid,  # Use just uid as display name
                    description="",
                    customProperties={
                        "type": ds_type,
                        "uid": ds_uid,
                        "full_path": dataset_name,  # Store full path in properties
                    },
                ),
                StatusClass(removed=False),
            ],
        )

        # Create schema metadata
        schema_metadata = self._create_schema_metadata(ds_uid, ds_type)
        if schema_metadata and schema_metadata.fields:
            dataset_snapshot.aspects.append(schema_metadata)

        self.report.report_dataset_scanned()
        yield MetadataWorkUnit(
            id=f"grafana-dataset-{ds_uid}",
            mce=MetadataChangeEvent(proposedSnapshot=dataset_snapshot),
        )

        yield MetadataChangeProposalWrapper(
            entityUrn=dataset_urn,
            aspect=SubTypesClass(
                typeNames=[
                    SubTypes.VIEW if self._has_sql_query(panel) else SubTypes.METRICS
                ]
            ),
        ).as_workunit()

    def _has_sql_query(self, panel: dict) -> bool:
        for target in panel.get("targets", []):
            if target.get("rawSql") or target.get("queryText"):
                return True
        return False

    def _create_schema_metadata(self, datasource_uid: str, datasource_type: str) -> SchemaMetadataClass:
        """Create schema metadata by aggregating fields from all panels using this datasource"""
        fields = []
        # Get all dashboards
        dashboards = self._get_dashboards()

        # Iterate through all panels in all dashboards
        for dashboard in dashboards:
            for panel in self._get_panels(dashboard):
                source = panel.get("datasource")
                if isinstance(source, dict) and source.get("uid") == datasource_uid:
                    fields.extend(self._get_chart_fields(panel))

        # Deduplicate fields based on fieldPath
        unique_fields = {field.fieldPath: field for field in fields}

        return SchemaMetadataClass(
            schemaName=f"{datasource_type}.{datasource_uid}",  # Use full path
            platform=builder.make_data_platform_urn(self.platform),
            version=0,
            fields=list(unique_fields.values()),
            hash="",
            platformSchema=OtherSchema(rawSchema=""),
        )

    def _emit_lineage(self, panel: dict) -> Optional[MetadataWorkUnit]:
        source = panel.get("datasource")
        if not source or not isinstance(source, dict):
            return None

        ds_type = source.get("type", "unknown")
        ds_uid = source.get("uid", "unknown")

        # Use full path for dataset name
        dataset_name = f"{ds_type}.{ds_uid}"

        # Get Grafana dataset URN
        ds_urn = builder.make_dataset_urn_with_platform_instance(
            platform=self.platform,
            name=dataset_name,
            platform_instance=self.platform_instance,
            env=self.env,
        )

        # Handle connection to platform mapping if configured
        if self.config.connection_to_platform_map and ds_uid in self.config.connection_to_platform_map:
            # Get upstream platform config
            platform_config = self.config.connection_to_platform_map[ds_uid]

            # Parse SQL if present
            parsed_sql = None
            for target in panel.get("targets", []):
                if "rawSql" in target:
                    parsed_sql = self._parse_sql_for_lineage(
                        sql=target["rawSql"],
                        source_platform=platform_config.platform,
                        database=platform_config.database,
                        platform_instance=platform_config.platform_instance,
                        env=platform_config.env or "PROD",
                    )
                    break
                elif "queryText" in target:  # InfluxDB
                    parsed_sql = self._parse_sql_for_lineage(
                        sql=target["queryText"],
                        source_platform=platform_config.platform,
                        database=platform_config.database,
                        platform_instance=platform_config.platform_instance,
                        env=platform_config.env or "PROD",
                    )
                    break

            if parsed_sql:
                return self._create_column_lineage(ds_urn, parsed_sql, platform_config)

            # If no SQL parsing, return basic lineage
            name = f"{platform_config.database}.{ds_uid}" if platform_config.database else ds_uid
            upstream_urn = builder.make_dataset_urn_with_platform_instance(
                platform=platform_config.platform,
                name=name,
                platform_instance=platform_config.platform_instance,
                env=platform_config.env or "PROD",
            )

            return MetadataChangeProposalWrapper(
                entityUrn=ds_urn,
                aspect=UpstreamLineageClass(
                    upstreams=[
                        UpstreamClass(
                            dataset=upstream_urn,
                            type=DatasetLineageTypeClass.TRANSFORMED
                        )
                    ]
                )
            ).as_workunit()
        return None

    def _get_chart_type(self, panel: dict) -> Optional[str]:
        """Map Grafana panel types to DataHub chart types"""
        type_mapping = {
            "graph": ChartTypeClass.LINE,
            "timeseries": ChartTypeClass.LINE,
            "table": ChartTypeClass.TABLE,
            "stat": ChartTypeClass.TEXT,
            "gauge": ChartTypeClass.TEXT,
            "bargauge": ChartTypeClass.TEXT,
            "bar": ChartTypeClass.BAR,
            "pie": ChartTypeClass.PIE,
            "heatmap": ChartTypeClass.TABLE,
            "histogram": ChartTypeClass.BAR,
        }

        panel_type = panel.get("type", "")
        return type_mapping.get(panel_type)

    def _get_custom_properties(self, panel: dict) -> Dict[str, str]:
        """Extract custom properties from panel"""
        props = {}

        # Add panel type
        panel_type = panel.get("type")
        if panel_type:
            props["type"] = panel_type

        # Add data source info
        source = panel.get("datasource")
        if source:
            if isinstance(source, dict):
                props["datasourceType"] = source.get("type", "")
                props["datasourceUid"] = source.get("uid", "")

        # Add other relevant properties
        for key in ["description", "format", "pluginVersion", "repeatDirection", "maxDataPoints"]:
            value = panel.get(key)
            if value:
                props[key] = str(value)

        targets = panel.get("targets", [])
        if targets:
            props["queryCount"] = str(len(targets))

        return props

    def _get_owner(self, dashboard_data: dict) -> Optional[OwnershipClass]:
        """Extract ownership information"""
        uid = dashboard_data.get("uid")
        if uid:
            owners = [
                OwnerClass(
                    owner=builder.make_user_urn(uid),
                    type=OwnershipTypeClass.TECHNICAL_OWNER
                )
            ]
            # Add dashboard UID as owner

            # Add creator if exists
            creator = dashboard_data.get("createdBy")
            if creator:
                owners.append(
                    OwnerClass(
                        owner=builder.make_user_urn(creator),
                        type=OwnershipTypeClass.DATAOWNER
                    )
                )

            return OwnershipClass(owners=owners)

        return None

    def _get_chart_fields(self, panel: dict) -> List[SchemaFieldClass]:
        """Extract fields used in the panel"""
        fields: List[SchemaFieldClass] = []

        # Extract fields from targets
        if "targets" in panel:
            for target in panel["targets"]:
                # Handle time series format
                format_type = target.get("format", "time_series")
                default_type = GrafanaTypeMapper.get_field_type(format_type)

                # Extract fields from different query types
                if "expr" in target:  # Prometheus style
                    fields.extend(self._extract_prometheus_fields(target, default_type))
                elif "rawSql" in target:  # SQL style
                    fields.extend(self._extract_sql_fields(target, default_type))
                elif "queryText" in target:  # InfluxDB style
                    fields.extend(self._extract_influx_fields(target, default_type))

        # Extract fields from transformations
        if "transformations" in panel:
            for transform in panel["transformations"]:
                if transform.get("type") == "organize":
                    fields.extend(self._extract_transform_fields(transform))

        return fields

    def _extract_prometheus_fields(
            self, target: dict, default_type: SchemaFieldDataTypeClass
    ) -> List[SchemaFieldClass]:
        fields = []
        expr = target.get("expr")
        if expr:
            # Basic metric name extraction
            metric_match = re.search(r'(\w+){', expr)
            if metric_match:
                fields.append(
                    SchemaFieldClass(
                        fieldPath=metric_match.group(1),
                        type=default_type,
                        description="Prometheus metric",
                        nativeDataType="prometheus_metric"
                    )
                )

            # Label extraction
            label_matches = re.findall(r'(\w+)="[^"]*"', expr)
            for label in label_matches:
                fields.append(
                    SchemaFieldClass(
                        fieldPath=label,
                        type=GrafanaTypeMapper.get_field_type("string"),
                        description="Prometheus label",
                        nativeDataType="prometheus_label"
                    )
                )
        return fields

    def _extract_sql_fields(self, target: dict, default_type: SchemaFieldDataTypeClass) -> List[SchemaFieldClass]:
        fields = []
        sql = target.get("rawSql")
        if not sql:
            return fields

        # Try SQL parsing first
        try:
            source = target.get("datasource", {})
            ds_type = source.get("type", "unknown")
            ds_uid = source.get("uid", "unknown")

            platform = self.platform
            database = None
            platform_instance = None
            env = "PROD"
            if (self.config.connection_to_platform_map and
                    isinstance(ds_uid, str) and
                    ds_uid in self.config.connection_to_platform_map):
                platform_config = self.config.connection_to_platform_map[ds_uid]
                platform = platform_config.platform
                database = platform_config.database
                platform_instance = platform_config.platform_instance
                env = platform_config.env

            parsed_sql = create_lineage_sql_parsed_result(
                query=sql,
                platform=platform,
                platform_instance=platform_instance,
                env=env,
                default_db=database,
                graph=self.ctx.graph,
            )

            if parsed_sql and parsed_sql.column_lineage:
                has_specific_columns = False
                seen_fields = set()

                for col_lineage in parsed_sql.column_lineage:
                    field_path = col_lineage.downstream.column

                    # Handle COUNT(*) specifically
                    if field_path.lower() == 'count(*)':
                        field_path = 'count'

                    if field_path != '*' and not field_path.endswith('.*'):
                        has_specific_columns = True
                        if field_path not in seen_fields:
                            seen_fields.add(field_path)
                            field_type = self._infer_sql_field_type(
                                field_path,
                                col_lineage.downstream.type if hasattr(col_lineage.downstream, 'type') else None,
                                sql
                            )
                            fields.append(
                                SchemaFieldClass(
                                    fieldPath=field_path,
                                    type=field_type or default_type,
                                    description=f"SQL column from {platform}",
                                    nativeDataType="sql_column"
                                )
                            )

                # Only add wildcard if no specific columns
                if not has_specific_columns:
                    fields.append(
                        SchemaFieldClass(
                            fieldPath='*',
                            type=default_type,
                            description="All columns",
                            nativeDataType="sql_column"
                        )
                    )
                return fields

        except Exception as e:
            logger.debug(f"SQL parsing failed for query: {sql}. Error: {str(e)}")

        # Fall back to regex parsing
        try:
            column_matches = re.findall(
                r'SELECT\s+(.*?)(?:FROM|WHERE|GROUP BY|ORDER BY|LIMIT|\)|\z)',
                sql,
                re.IGNORECASE | re.DOTALL
            )

            has_specific_columns = False
            seen_fields = set()

            for match in column_matches:
                in_parentheses = 0
                current_column = ""
                columns = []

                for char in match:
                    if char == '(':
                        in_parentheses += 1
                    elif char == ')':
                        in_parentheses -= 1
                    elif char == ',' and in_parentheses == 0:
                        columns.append(current_column.strip())
                        current_column = ""
                        continue
                    current_column += char

                if current_column.strip():
                    columns.append(current_column.strip())

                for col in columns:
                    col = col.strip()
                    field_name = col

                    # Handle COUNT(*) specifically
                    if field_name.lower() == 'count(*)':
                        field_name = 'count'
                    # Skip other wildcards
                    elif field_name == '*' or field_name.endswith('.*'):
                        continue

                    has_specific_columns = True

                    if ' as ' in col.lower():
                        field_name = col.split(' as ')[-1].strip().strip('"\'`[]')
                    elif ' ' in col:
                        field_name = col.split()[-1].strip().strip('"\'`[]')

                    field_name = re.sub(r'["\'\`\[\]]', '', field_name)

                    if field_name not in seen_fields:
                        seen_fields.add(field_name)
                        field_type = self._infer_sql_field_type(field_name, None, sql)

                        fields.append(
                            SchemaFieldClass(
                                fieldPath=field_name,
                                type=field_type or default_type,
                                description="SQL column",
                                nativeDataType="sql_column"
                            )
                        )

            # Only add wildcard if no specific columns found
            if not has_specific_columns:
                fields.append(
                    SchemaFieldClass(
                        fieldPath='*',
                        type=default_type,
                        description="All columns",
                        nativeDataType="sql_column"
                    )
                )

        except Exception as e:
            logger.debug(f"Regex parsing failed for query: {sql}. Error: {str(e)}")

        return fields

    def _infer_sql_field_type(
            self,
            field_name: str,
            sql_type: Optional[str] = None,
            sql_context: Optional[str] = None
    ) -> SchemaFieldDataTypeClass:
        """
        Infer field type from SQL context and common patterns
        """
        # If we have explicit SQL type, use it
        if sql_type:
            if 'int' in sql_type.lower():
                return SchemaFieldDataTypeClass(type=NumberTypeClass())
            elif 'time' in sql_type.lower() or 'date' in sql_type.lower():
                return SchemaFieldDataTypeClass(type=TimeTypeClass())
            elif 'bool' in sql_type.lower():
                return SchemaFieldDataTypeClass(type=BooleanTypeClass())
            elif any(x in sql_type.lower() for x in ['float', 'double', 'decimal', 'numeric']):
                return SchemaFieldDataTypeClass(type=NumberTypeClass())

        # Check field name patterns
        field_lower = field_name.lower()
        if any(x in field_lower for x in ['time', 'date', 'timestamp', 'created', 'updated']):
            return SchemaFieldDataTypeClass(type=TimeTypeClass())
        elif any(x in field_lower for x in ['count', 'sum', 'avg', 'min', 'max', 'total', 'amount', 'number']):
            return SchemaFieldDataTypeClass(type=NumberTypeClass())
        elif any(x in field_lower for x in ['is_', 'has_', 'flag', 'enabled', 'active']):
            return SchemaFieldDataTypeClass(type=BooleanTypeClass())

        # Check SQL context if available
        if sql_context:
            sql_lower = sql_context.lower()
            field_pattern = re.escape(field_name.lower())

            # Check for aggregate functions
            if re.search(rf'(count|sum|avg|min|max)\s*\(\s*{field_pattern}\s*\)', sql_lower):
                return SchemaFieldDataTypeClass(type=NumberTypeClass())
            # Check for date/time functions
            elif re.search(rf'(date|time|timestamp)\s*\(\s*{field_pattern}\s*\)', sql_lower):
                return SchemaFieldDataTypeClass(type=TimeTypeClass())
            # Check for numeric operations
            elif re.search(rf'{field_pattern}\s*[+\-*/]' , sql_lower) or \
                    re.search(rf'[+\-*/]\s*{field_pattern}' , sql_lower):
                return SchemaFieldDataTypeClass(type=NumberTypeClass())

        # Default to string if no type could be inferred
        return SchemaFieldDataTypeClass(type=StringTypeClass())

    def _extract_influx_fields(
            self, target: dict, default_type: SchemaFieldDataTypeClass
    ) -> List[SchemaFieldClass]:
        fields = []
        query = target.get("queryText")
        if query:
            # Extract measurement name
            measurement_match = re.search(r'FROM\s+"([^"]+)"', query, re.IGNORECASE)
            if measurement_match:
                fields.append(
                    SchemaFieldClass(
                        fieldPath=measurement_match.group(1),
                        type=default_type,
                        description="InfluxDB measurement",
                        nativeDataType="influx_measurement"
                    )
                )

            # Extract field keys
            field_matches = re.findall(r'SELECT\s+([^FROM]+)', query, re.IGNORECASE)
            if field_matches:
                field_list = field_matches[0].split(',')
                for field in field_list:
                    field = field.strip()
                    fields.append(
                        SchemaFieldClass(
                            fieldPath=field,
                            type=default_type,
                            description="InfluxDB field",
                            nativeDataType="influx_field"
                        )
                    )
        return fields

    def _extract_transform_fields(
            self, transform: dict
    ) -> List[SchemaFieldClass]:
        """Extract fields from transformations"""
        fields = []
        if transform.get("type") == "organize":
            for field_name, config in transform.get("options", {}).get("indexByName", {}).items():
                transform_type = config.get("type", "string")
                fields.append(
                    SchemaFieldClass(
                        fieldPath=field_name,
                        type=GrafanaTypeMapper.get_field_type(transform_type),
                        nativeDataType=GrafanaTypeMapper.get_native_type(transform_type),
                        description=f"Transformed field via {transform.get('type')} operation"
                    )
                )
        return fields

    def _get_field_tags(self, field: SchemaFieldClass) -> List[str]:
        """Determine appropriate tags for fields based on type"""
        tags = []

        if self.config.auto_tag_dimensions and isinstance(
                field.type.type,
                (StringTypeClass, BooleanTypeClass)
        ):
            tags.append(builder.make_tag_urn("Dimension"))

        if self.config.auto_tag_measures and isinstance(
                field.type.type,
                (NumberTypeClass, TimeTypeClass)
        ):
            tags.append(builder.make_tag_urn("Measure"))

        return tags

    def _create_data_platform_instance(self) -> DataPlatformInstanceClass:
        return DataPlatformInstanceClass(
            platform=f"urn:li:dataPlatform:{self.platform}",
            instance=(
                builder.make_dataplatform_instance_urn(self.platform, self.platform_instance)
                if self.platform_instance
                else None
            ),
        )

    def _create_dashboard_container(self, dashboard_data: dict, folder_id: Optional[str] = None) -> Iterable[
        MetadataWorkUnit]:
        dashboard_uid = dashboard_data.get("uid")
        if not dashboard_uid:
            return

        dashboard_key = DashboardContainerKey(
            platform=self.platform,
            instance=self.platform_instance,
            dashboard_id=dashboard_uid,
        )

        parent_key = None
        if folder_id:
            parent_key = FolderKey(
                platform=self.platform,
                instance=self.platform_instance,
                folder_id=str(folder_id),
            )

        yield from gen_containers(
            container_key=dashboard_key,
            name=dashboard_data.get("title", ""),
            sub_types=["Dashboard"],
            description=dashboard_data.get("description", ""),
            parent_container_key=parent_key,
        )

    def _get_dataset_subtype(self, source_type: str) -> List[str]:
        """Get dataset subtype based on source type"""
        if source_type == "prometheus":
            return ["Metrics"]
        return ["Query"]

    def _parse_sql_for_lineage(
        self,
        sql: str,
        source_platform: str,
        database: Optional[str],
        platform_instance: Optional[str],
        env: Optional[str],
    ) -> Optional[SqlParsingResult]:
        """Parse SQL query to extract lineage information"""
        try:
            if self.ctx.graph:
                return create_lineage_sql_parsed_result(
                    query=sql,
                    platform=source_platform,
                    platform_instance=platform_instance,
                    env=env or "PROD",
                    default_db=database,
                    graph=self.ctx.graph,
                )
            else:
                self.report.report_warning(
                    message="No DataHub specified for graph",
                    context=sql,
                )
        except Exception as e:
            self.report.report_warning(
                message="Failed to parse SQL query",
                context=sql,
                exc=e,
            )
            return None

    def _create_column_lineage(
            self,
            dataset_urn: str,
            parsed_sql: SqlParsingResult,
            platform_config: PlatformConnectionConfig
    ) -> Optional[MetadataWorkUnit]:
        """Create column-level lineage for dataset"""
        if not parsed_sql.column_lineage:
            return None

        upstream_lineages = []
        for col_lineage in parsed_sql.column_lineage:
            upstream_lineages.append(
                FineGrainedLineageClass(
                    downstreamType=FineGrainedLineageDownstreamTypeClass.FIELD,
                    downstreams=[
                        builder.make_schema_field_urn(
                            dataset_urn,
                            col_lineage.downstream.column
                        )
                    ],
                    upstreamType=FineGrainedLineageUpstreamTypeClass.FIELD_SET,
                    upstreams=[
                        builder.make_schema_field_urn(
                            upstream_dataset,
                            col.column
                        )
                        for col in col_lineage.upstreams
                        for upstream_dataset in parsed_sql.in_tables
                    ]
                )
            )

        # Use platform config for upstream dataset information
        return MetadataChangeProposalWrapper(
            entityUrn=dataset_urn,
            aspect=UpstreamLineageClass(
                upstreams=[
                    UpstreamClass(
                        dataset=table,
                        type=DatasetLineageTypeClass.TRANSFORMED
                    )
                    for table in parsed_sql.in_tables
                ],
                fineGrainedLineages=upstream_lineages
            )
        ).as_workunit()

    def extract_chart_schema_from_params(self, panel: dict) -> List[SchemaFieldClass]:
        fields = []

        # Extract columns from chart parameters
        if panel.get("fieldConfig", {}).get("defaults", {}).get("custom", {}).get("axisLabel"):
            fields.append(SchemaFieldClass(
                fieldPath=panel["fieldConfig"]["defaults"]["custom"]["axisLabel"],
                type=SchemaFieldDataTypeClass(type=StringTypeClass()),
                nativeDataType="string"
            ))

        # Extract filter columns
        for target in panel.get("targets", []):
            if target.get("filters"):
                for filter_col in target["filters"]:
                    fields.append(SchemaFieldClass(
                        fieldPath=filter_col["key"],
                        type=SchemaFieldDataTypeClass(type=StringTypeClass()),
                        nativeDataType="string"
                    ))

        return fields

    def get_report(self) -> GrafanaSourceReport:
        return self.report

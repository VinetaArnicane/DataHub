{
  "urn:li:dataFlow:(spark,PythonHdfsIn2HiveCreateInsertTable,spark_spark-master_7077)": {
    "value": {
      "com.linkedin.metadata.snapshot.DataFlowSnapshot": {
        "urn": "urn:li:dataFlow:(spark,PythonHdfsIn2HiveCreateInsertTable,spark_spark-master_7077)",
        "aspects": [
          {
            "com.linkedin.metadata.key.DataFlowKey": {
              "orchestrator": "spark",
              "cluster": "spark_spark-master_7077",
              "flowId": "PythonHdfsIn2HiveCreateInsertTable"
            }
          },
          {
            "com.linkedin.datajob.DataFlowInfo": {
              "name": "PythonHdfsIn2HiveCreateInsertTable",
              "customProperties": {
                "sparkUser": "root",
                "appName": "PythonHdfsIn2HiveCreateInsertTable"
              }
            }
          },
          {
            "com.linkedin.common.DataPlatformInstance": {
              "platform": "urn:li:dataPlatform:spark"
            }
          },
          {
            "com.linkedin.common.BrowsePaths": {
              "paths": [
                "/spark/spark_spark-master_7077/pythonhdfsin2hivecreateinserttable"
              ]
            }
          }
        ]
      }
    }
  },
  "urn:li:dataJob:(urn:li:dataFlow:(spark,PythonHdfsIn2HiveCreateInsertTable,spark_spark-master_7077),QueryExecId_5)": {
    "value": {
      "com.linkedin.metadata.snapshot.DataJobSnapshot": {
        "urn": "urn:li:dataJob:(urn:li:dataFlow:(spark,PythonHdfsIn2HiveCreateInsertTable,spark_spark-master_7077),QueryExecId_5)",
        "aspects": [
          {
            "com.linkedin.metadata.key.DataJobKey": {
              "jobId": "QueryExecId_5",
              "flow": "urn:li:dataFlow:(spark,PythonHdfsIn2HiveCreateInsertTable,spark_spark-master_7077)"
            }
          },
          {
            "com.linkedin.common.DataPlatformInstance": {
              "platform": "urn:li:dataPlatform:spark"
            }
          },
          {
            "com.linkedin.datajob.DataJobInputOutput": {
              "inputDatasets": [
                "urn:li:dataset:(urn:li:dataPlatform:hdfs,file:/opt/workspace/resources/data/in1.csv,PROD)",
                "urn:li:dataset:(urn:li:dataPlatform:hdfs,file:/opt/workspace/resources/data/in2.csv,PROD)"
              ],
              "outputDatasets": [
                "urn:li:dataset:(urn:li:dataPlatform:hive,PythonHdfsIn2HiveCreateInsertTable.foo4,PROD)"
              ]
            }
          },
          {
            "com.linkedin.common.BrowsePaths": {
              "paths": [
                "/spark/pythonhdfsin2hivecreateinserttable/queryexecid_5"
              ]
            }
          },
          {
            "com.linkedin.datajob.DataJobInfo": {
              "name": "saveAsTable at NativeMethodAccessorImpl.java:0",
              "type": {
                "string": "sparkJob"
              },
              "customProperties": {
                "SQLQueryId": "5",
                "appName": "PythonHdfsIn2HiveCreateInsertTable",
                "description": "saveAsTable at NativeMethodAccessorImpl.java:0",
                "queryPlan": "CreateDataSourceTableAsSelectCommand `PythonHdfsIn2HiveCreateInsertTable`.`foo4`, Overwrite, [a, b, c, d]\n+- Project [a#16, b#20, c#40, d#44]\n   +- Join Inner, (id#10 = id#34)\n      :- Project [id#10, c1#11 AS a#16, c2#12 AS b#20]\n      :  +- Filter isnotnull(id#10)\n      :     +- Relation[id#10,c1#11,c2#12] csv\n      +- Project [id#34, c1#35 AS c#40, c2#36 AS d#44]\n         +- Filter isnotnull(id#34)\n            +- Relation[id#34,c1#35,c2#36] csv\n"
              }
            }
          }
        ]
      }
    }
  },
  "urn:li:dataJob:(urn:li:dataFlow:(spark,PythonHdfsIn2HiveCreateInsertTable,spark_spark-master_7077),QueryExecId_6)": {
    "value": {
      "com.linkedin.metadata.snapshot.DataJobSnapshot": {
        "urn": "urn:li:dataJob:(urn:li:dataFlow:(spark,PythonHdfsIn2HiveCreateInsertTable,spark_spark-master_7077),QueryExecId_6)",
        "aspects": [
          {
            "com.linkedin.metadata.key.DataJobKey": {
              "jobId": "QueryExecId_6",
              "flow": "urn:li:dataFlow:(spark,PythonHdfsIn2HiveCreateInsertTable,spark_spark-master_7077)"
            }
          },
          {
            "com.linkedin.common.BrowsePaths": {
              "paths": [
                "/spark/pythonhdfsin2hivecreateinserttable/queryexecid_6"
              ]
            }
          },
          {
            "com.linkedin.datajob.DataJobInfo": {
              "name": "saveAsTable at NativeMethodAccessorImpl.java:0",
              "type": {
                "string": "sparkJob"
              },
              "customProperties": {
                "SQLQueryId": "6",
                "appName": "PythonHdfsIn2HiveCreateInsertTable",
                "description": "saveAsTable at NativeMethodAccessorImpl.java:0",
                "queryPlan": "CreateDataSourceTableAsSelectCommand `pythonhdfsin2hivecreateinserttable`.`foo4`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, Append, [a, b, c, d]\n+- Project [a#16, b#20, c#40, d#44]\n   +- Join Inner, (id#10 = id#34)\n      :- Project [id#10, c1#11 AS a#16, c2#12 AS b#20]\n      :  +- Filter isnotnull(id#10)\n      :     +- Relation[id#10,c1#11,c2#12] csv\n      +- Project [id#34, c1#35 AS c#40, c2#36 AS d#44]\n         +- Filter isnotnull(id#34)\n            +- Relation[id#34,c1#35,c2#36] csv\n"
              }
            }
          },
          {
            "com.linkedin.datajob.DataJobInputOutput": {
              "inputDatasets": [
                "urn:li:dataset:(urn:li:dataPlatform:hdfs,file:/opt/workspace/resources/data/in1.csv,PROD)",
                "urn:li:dataset:(urn:li:dataPlatform:hdfs,file:/opt/workspace/resources/data/in2.csv,PROD)"
              ],
              "outputDatasets": [
                "urn:li:dataset:(urn:li:dataPlatform:hive,pythonhdfsin2hivecreateinserttable.foo4,PROD)"
              ]
            }
          },
          {
            "com.linkedin.common.DataPlatformInstance": {
              "platform": "urn:li:dataPlatform:spark"
            }
          }
        ]
      }
    }
  },
  "urn:li:dataJob:(urn:li:dataFlow:(spark,PythonHdfsIn2HiveCreateInsertTable,spark_spark-master_7077),QueryExecId_7)": {
    "value": {
      "com.linkedin.metadata.snapshot.DataJobSnapshot": {
        "urn": "urn:li:dataJob:(urn:li:dataFlow:(spark,PythonHdfsIn2HiveCreateInsertTable,spark_spark-master_7077),QueryExecId_7)",
        "aspects": [
          {
            "com.linkedin.metadata.key.DataJobKey": {
              "jobId": "QueryExecId_7",
              "flow": "urn:li:dataFlow:(spark,PythonHdfsIn2HiveCreateInsertTable,spark_spark-master_7077)"
            }
          },
          {
            "com.linkedin.datajob.DataJobInputOutput": {
              "inputDatasets": [
                "urn:li:dataset:(urn:li:dataPlatform:hdfs,file:/opt/workspace/resources/data/in1.csv,PROD)",
                "urn:li:dataset:(urn:li:dataPlatform:hdfs,file:/opt/workspace/resources/data/in2.csv,PROD)"
              ],
              "outputDatasets": [
                "urn:li:dataset:(urn:li:dataPlatform:hive,pythonhdfsin2hivecreateinserttable.foo4,PROD)"
              ]
            }
          },
          {
            "com.linkedin.common.BrowsePaths": {
              "paths": [
                "/spark/pythonhdfsin2hivecreateinserttable/queryexecid_7"
              ]
            }
          },
          {
            "com.linkedin.datajob.DataJobInfo": {
              "name": "insertInto at NativeMethodAccessorImpl.java:0",
              "type": {
                "string": "sparkJob"
              },
              "customProperties": {
                "SQLQueryId": "7",
                "appName": "PythonHdfsIn2HiveCreateInsertTable",
                "description": "insertInto at NativeMethodAccessorImpl.java:0",
                "queryPlan": "InsertIntoHadoopFsRelationCommand file:/opt/workspace/python-spark-lineage-test/spark-warehouse/pythonhdfsin2hivecreateinserttable.db/foo4, false, Parquet, Map(serialization.format -> 1, path -> file:/opt/workspace/python-spark-lineage-test/spark-warehouse/pythonhdfsin2hivecreateinserttable.db/foo4), Append, CatalogTable(\nDatabase: pythonhdfsin2hivecreateinserttable\nTable: foo4\nOwner: root\nCreated Time: Tue Jan 11 07:20:17 UTC 2022\nLast Access: Thu Jan 01 00:00:00 UTC 1970\nCreated By: Spark 2.4.8\nType: MANAGED\nProvider: parquet\nTable Properties: [transient_lastDdlTime=1641885617]\nLocation: file:/opt/workspace/python-spark-lineage-test/spark-warehouse/pythonhdfsin2hivecreateinserttable.db/foo4\nSerde Library: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\nInputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\nOutputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\nStorage Properties: [serialization.format=1]\nSchema: root\n |-- a: string (nullable = true)\n |-- b: string (nullable = true)\n |-- c: string (nullable = true)\n |-- d: string (nullable = true)\n), org.apache.spark.sql.execution.datasources.InMemoryFileIndex@d1ec7f98, [a, b, c, d]\n+- Project [a#16, b#20, c#40, d#44]\n   +- Join Inner, (id#10 = id#34)\n      :- Project [id#10, c1#11 AS a#16, c2#12 AS b#20]\n      :  +- Filter isnotnull(id#10)\n      :     +- Relation[id#10,c1#11,c2#12] csv\n      +- Project [id#34, c1#35 AS c#40, c2#36 AS d#44]\n         +- Filter isnotnull(id#34)\n            +- Relation[id#34,c1#35,c2#36] csv\n"
              }
            }
          },
          {
            "com.linkedin.common.DataPlatformInstance": {
              "platform": "urn:li:dataPlatform:spark"
            }
          }
        ]
      }
    }
  }
}